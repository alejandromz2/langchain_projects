{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Numero 43', 'Numero 43']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "paso1 = RunnableLambda(lambda x: f\"Numero {x}\")\n",
    "\n",
    "def duplicar_text(texto):\n",
    "    return [texto]*2\n",
    "\n",
    "paso2 = RunnableLambda(duplicar_text)\n",
    "\n",
    "cadena = paso1 | paso2\n",
    "\n",
    "resultado = cadena.invoke(43)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuración Inicial\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocesador de Texto\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Limpia el texto eliminando espacios extras y limitando longitud\"\"\"\n",
    "    # Pista: usa .strip() para eliminar espacios\n",
    "    # Pista: limita a 500 caracteres con slicing [:500]\n",
    "    return text.strip()[:500] # ¡Completa aquí!\n",
    " \n",
    "# Convertir la función en un Runnable\n",
    "preprocessor = RunnableLambda(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Generador de Resúmenes\n",
    "def generate_summary(text):\n",
    "    \"\"\"Genera un resumen conciso del texto\"\"\"\n",
    "    prompt = f\"\"\"Resume en una sola oración considerando la idea\n",
    "    principal del texto. Maneja oraciones de menos de 500 palabras : {text}\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "summary_branch = RunnableLambda(generate_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Analizador de Sentimientos\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analiza el sentimiento y devuelve resultado estructurado\"\"\"\n",
    "    prompt = f\"\"\"Analiza el sentimiento del siguiente texto.\n",
    "    Responde ÚNICAMENTE en formato JSON válido:\n",
    "    {{\"sentimiento\": \"positivo|negativo|neutro\", \"razon\": \"justificación breve\"}}\n",
    "    \n",
    "    Texto: {text}\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    try:\n",
    "        return json.loads(response.content)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"sentimiento\": \"neutro\", \"razon\": \"Error en análisis\"}\n",
    "    \n",
    "sentiment_branch = RunnableLambda(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Función de Combinación\n",
    "\n",
    "def merge_results(data):\n",
    "    \"\"\"Combina los resultados de ambas ramas en un formato unificado\"\"\"\n",
    "    return {\n",
    "        \"resumen\": data[\"resumen\"],\n",
    "        \"sentimiento\": data[\"sentimiento_data\"][\"sentimiento\"],\n",
    "        \"razon\": data[\"sentimiento_data\"][\"razon\"]\n",
    "    }\n",
    "\n",
    "merger = RunnableLambda(merge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Función de Procesamiento Principal\n",
    "\n",
    "paralel_analysis = RunnableParallel({\n",
    "    \"resumen\":summary_branch,\n",
    "    \"sentimiento_data\":sentiment_branch\n",
    "})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Construcción de la Cadena Final\n",
    "chain = preprocessor | paralel_analysis | merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: ¡Me encanta este producto! Funciona perfectamente y llegó muy rápido.\n",
      "Resultado: {'resumen': 'Este producto es excelente, ya que funciona a la perfección y llegó rápidamente.', 'sentimiento': 'positivo', 'razon': 'El usuario expresa satisfacción con el producto y destaca su buen funcionamiento y rapidez en la entrega.'}\n",
      "--------------------------------------------------\n",
      "Texto: El servicio al cliente fue terrible, nadie me ayudó con mi problema.\n",
      "Resultado: {'resumen': 'El servicio al cliente fue deficiente, ya que no recibí la ayuda necesaria para resolver mi problema.', 'sentimiento': 'negativo', 'razon': 'El texto expresa insatisfacción con el servicio al cliente y menciona que no recibió ayuda.'}\n",
      "--------------------------------------------------\n",
      "Texto: El clima está nublado hoy, probablemente llueva más tarde.\n",
      "Resultado: {'resumen': 'Hoy el clima es nublado y es probable que llueva más tarde.', 'sentimiento': 'neutro', 'razon': 'El texto describe una condición climática sin expresar emociones positivas o negativas.'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Prueba con diferentes textos\n",
    "textos_prueba = [\n",
    "    \"¡Me encanta este producto! Funciona perfectamente y llegó muy rápido.\",\n",
    "    \"El servicio al cliente fue terrible, nadie me ayudó con mi problema.\",\n",
    "    \"El clima está nublado hoy, probablemente llueva más tarde.\"\n",
    "]\n",
    " \n",
    "for texto in textos_prueba:\n",
    "    resultado = chain.invoke(texto)\n",
    "    print(f\"Texto: {texto}\")\n",
    "    print(f\"Resultado: {resultado}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'resumen': 'El producto es excelente, funciona a la perfección y llegó rápidamente.', 'sentimiento': 'positivo', 'razon': 'El usuario expresa satisfacción y aprecio por el producto y su entrega.'}, {'resumen': 'El servicio al cliente fue deficiente, ya que no recibí la ayuda necesaria para resolver mi problema.', 'sentimiento': 'negativo', 'razon': 'El texto expresa insatisfacción con el servicio al cliente, indicando que no recibió la ayuda necesaria.'}, {'resumen': 'Hoy el clima es nublado y es probable que llueva más tarde.', 'sentimiento': 'neutro', 'razon': 'El texto describe una condición climática sin expresar emociones positivas o negativas.'}]\n"
     ]
    }
   ],
   "source": [
    "resultado_batch = chain.batch(textos_prueba)\n",
    "print(resultado_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probar Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Eres un experto de marketing. Sugiere un eslogan  creativo para un producto café organico\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \" Eres un experto de marketing. Sugiere un eslogan  creativo para un producto {producto}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"producto\"]\n",
    ")\n",
    "\n",
    "prompt_lleno = prompt.format(producto = \"café organico\")\n",
    "print(prompt_lleno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.system.SystemMessage'>: Eres un traductor del español al inglés muy precios.\n",
      "<class 'langchain_core.messages.human.HumanMessage'>: Hola mundo, ¿cómo estás?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Eres un traductor del español al inglés muy precios.\"),\n",
    "    (\"human\",\"{texto}\")\n",
    "])\n",
    "\n",
    "mensajes = chat_prompt.format_messages(texto=\"Hola mundo, ¿cómo estás?\")\n",
    "\n",
    "for m in mensajes:\n",
    "    print(f\"{type(m)}: {m.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eres un asistente util que mantiene el contexto de la conversación\n",
      "Usuario: ¿Cual es la capital de Francia?\n",
      "IA: La capital de Francia es París.\n",
      "Usuario: ¿Y cuantos habitantes tiene?\n",
      "IA: Paris tiene aproximadamente 2.2 millones de habitantes en la ciudad.\n",
      "Usuario: ¿Puedes contarme algo sobre su arquitectura?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente util que mantiene el contexto de la conversación\"),\n",
    "    MessagesPlaceholder(variable_name=\"historial\"),\n",
    "    (\"human\",\"Usuario: {pregunta_actual}\")\n",
    "])\n",
    "historial_conversacion = [\n",
    "    HumanMessage(content=\"Usuario: ¿Cual es la capital de Francia?\"),\n",
    "    AIMessage(content=\"IA: La capital de Francia es París.\"),\n",
    "    HumanMessage(content=\"Usuario: ¿Y cuantos habitantes tiene?\"),\n",
    "    AIMessage(content=\"IA: Paris tiene aproximadamente 2.2 millones de habitantes en la ciudad.\"),\n",
    "]\n",
    "\n",
    "mensajes = chat_prompt.format_messages(\n",
    "    historial = historial_conversacion,\n",
    "    pregunta_actual = \"¿Puedes contarme algo sobre su arquitectura?\"\n",
    ")\n",
    "\n",
    "for m in mensajes:\n",
    "    print(m.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando Few-Shot\n",
    "Es una técnica de In-Context Learning donde proporcionamos al modelo entre 2-10 ejemplos de la tarea que queremos que realice. Estos ejemplos sirven como \"entrenamiento instantáneo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejores Prácticas\n",
    "\n",
    "- 2-5 ejemplos: Suficiente para mostrar el patrón, no demasiados\n",
    "\n",
    "- Ejemplos diversos: Cubrir diferentes casos/variaciones\n",
    "\n",
    "- Formato consistente: Mismo patrón en todos los ejemplos\n",
    "\n",
    "- Ejemplos de calidad: Outputs perfectos como referencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MessagesPlaceholder transforma los few-shot examples de una técnica básica a una herramienta poderosa y flexible. La estructura clara de mensajes Human/AI hace que el modelo comprenda mejor qué esperamos, resultando en respuestas más precisas y consistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensaje 1 (SystemMessage):\n",
      "Eres un experto en análisis de sentimientos. Clasifica cada texto como: POSITIVO, NEGATIVO o NEUTRO.\n",
      "----------------------------------------\n",
      "Mensaje 2 (HumanMessage):\n",
      "Texto a analizar: Me encanta este producto, es increíble\n",
      "----------------------------------------\n",
      "Mensaje 3 (AIMessage):\n",
      "POSITIVO\n",
      "----------------------------------------\n",
      "Mensaje 4 (HumanMessage):\n",
      "Texto a analizar: El servicio fue terrible, muy decepcionante\n",
      "----------------------------------------\n",
      "Mensaje 5 (AIMessage):\n",
      "NEGATIVO\n",
      "----------------------------------------\n",
      "Mensaje 6 (HumanMessage):\n",
      "Texto a analizar: El clima está nublado hoy\n",
      "----------------------------------------\n",
      "Mensaje 7 (AIMessage):\n",
      "NEUTRO\n",
      "----------------------------------------\n",
      "Mensaje 8 (HumanMessage):\n",
      "Texto a analizar: ¡Qué día tan maravilloso!\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    " \n",
    "# Template para clasificación de sentimientos con few-shot examples\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un experto en análisis de sentimientos. Clasifica cada texto como: POSITIVO, NEGATIVO o NEUTRO.\"),\n",
    "    MessagesPlaceholder(variable_name=\"ejemplos\"),\n",
    "    (\"human\", \"Texto a analizar: {texto_usuario}\")\n",
    "])\n",
    " \n",
    "# Few-shot examples para análisis de sentimientos\n",
    "ejemplos_sentimientos = [\n",
    "    HumanMessage(content=\"Texto a analizar: Me encanta este producto, es increíble\"),\n",
    "    AIMessage(content=\"POSITIVO\"),\n",
    "    HumanMessage(content=\"Texto a analizar: El servicio fue terrible, muy decepcionante\"),\n",
    "    AIMessage(content=\"NEGATIVO\"),\n",
    "    HumanMessage(content=\"Texto a analizar: El clima está nublado hoy\"),\n",
    "    AIMessage(content=\"NEUTRO\")\n",
    "]\n",
    " \n",
    "# Generar el prompt con los ejemplos\n",
    "mensajes = chat_prompt.format_messages(\n",
    "    ejemplos=ejemplos_sentimientos,\n",
    "    texto_usuario=\"¡Qué día tan maravilloso!\"\n",
    ")\n",
    " \n",
    "# Ver el resultado\n",
    "for i, m in enumerate(mensajes):\n",
    "    print(f\"Mensaje {i+1} ({m.__class__.__name__}):\")\n",
    "    print(m.content)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rol Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eres un nutricionista especializado en dietas veganas. Responde de manera profesional pero accesible\n",
      "Mi pregunta sobre proteina vegetales es: ¿Cuales son las mejores fuentes de proteinas veganas para un atleta profesional?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "plantilla_sistema = SystemMessagePromptTemplate.from_template(\"\" \\\n",
    "\"Eres un {rol} especializado en {especialidad}. Responde de manera {tono}\"\n",
    ")\n",
    "\n",
    "plantilla_humano = HumanMessagePromptTemplate.from_template(\n",
    "    \"Mi pregunta sobre {tema} es: {pregunta}\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    plantilla_sistema,\n",
    "    plantilla_humano\n",
    "])\n",
    "\n",
    "mensajes = chat_prompt.format_messages(\n",
    "    rol = \"nutricionista\",\n",
    "    especialidad = \"dietas veganas\",\n",
    "    tono = \"profesional pero accesible\",\n",
    "    tema= \"proteina vegetales\",\n",
    "    pregunta=\"¿Cuales son las mejores fuentes de proteinas veganas para un atleta profesional?\"\n",
    ")\n",
    "\n",
    "for m in mensajes:\n",
    "    print(m.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":123,\"nombre\":\"Ana\",\"activo\":true}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Usuario(BaseModel):\n",
    "    id: int\n",
    "    nombre: str\n",
    "    activo: bool = True\n",
    "\n",
    "\n",
    "data = {\"id\": \"123\", \"nombre\":\"Ana\"}\n",
    "\n",
    "usuario = Usuario(**data)\n",
    "\n",
    "print(usuario.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output parses en Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"resumen\":\"El hablante expresa su entusiasmo por una nueva película de acción, destacando sus efectos especiales.\",\"sentimientos\":\"Positivo\"}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class AnalisisTexto(BaseModel):\n",
    "    resumen: str = Field(description=\"Resumen breve del texto.\")\n",
    "    sentimientos: str = Field(description=\"Sentimientos del texto (Positivo, neutro o negativo\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.6)\n",
    "\n",
    "structured_llm = llm.with_structured_output(AnalisisTexto)\n",
    "\n",
    "texto_prueba = \"Me encantó la nueva pelicula de acción, tiene muchos efectos especiales\"\n",
    "\n",
    "resultado = structured_llm.invoke(f'Analiza el siguiente exto: {texto_prueba}')\n",
    "\n",
    "print(resultado.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output parses antiguos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrucciones generadas:\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"resumen\": {\"description\": \"Resumen breve del texto\", \"title\": \"Resumen\", \"type\": \"string\"}, \"sentimiento\": {\"description\": \"Sentimiento del texto (Positivo, neutro o negativo)\", \"title\": \"Sentimiento\", \"type\": \"string\"}, \"palabras_clave\": {\"description\": \"Lista de palabras clave del texto\", \"items\": {\"type\": \"string\"}, \"title\": \"Palabras Clave\", \"type\": \"array\"}}, \"required\": [\"resumen\", \"sentimiento\", \"palabras_clave\"]}\n",
      "```\n",
      "=== RESULTADO DEL ANÁLISIS ===\n",
      "Resumen: La película de ciencia ficción 'Estrella Galáctica' destaca por sus impresionantes efectos visuales, una trama tensa y actuaciones convincentes de los actores principales.\n",
      "Sentimiento: Positivo\n",
      "Palabras clave: Estrella Galáctica, ciencia ficción, efectos visuales, trama, actuaciones, mejores películas\n",
      "\n",
      "=== JSON RESULTANTE ===\n",
      "{\n",
      "  \"resumen\": \"La película de ciencia ficción 'Estrella Galáctica' destaca por sus impresionantes efectos visuales, una trama tensa y actuaciones convincentes de los actores principales.\",\n",
      "  \"sentimiento\": \"Positivo\",\n",
      "  \"palabras_clave\": [\n",
      "    \"Estrella Galáctica\",\n",
      "    \"ciencia ficción\",\n",
      "    \"efectos visuales\",\n",
      "    \"trama\",\n",
      "    \"actuaciones\",\n",
      "    \"mejores películas\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Tipo de objeto: <class '__main__.AnalisisTexto'>\n",
      "Tipo de diccionario: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    " \n",
    "class AnalisisTexto(BaseModel):\n",
    "    resumen: str = Field(description=\"Resumen breve del texto\")\n",
    "    sentimiento: str = Field(description=\"Sentimiento del texto (Positivo, neutro o negativo)\")\n",
    "    palabras_clave: list[str] = Field(description=\"Lista de palabras clave del texto\")\n",
    "\n",
    "# Crear el parser con nuestro modelo\n",
    "parser = PydanticOutputParser(pydantic_object=AnalisisTexto)\n",
    " \n",
    "# Ver las instrucciones que genera automáticamente\n",
    "print(\"Instrucciones generadas:\")\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Eres un experto analista de texto. Analiza el siguiente texto con mucho cuidado y proporciona un análisis detallado.\n",
    " \n",
    "{format_instructions}\n",
    " \n",
    "Texto a analizar:\n",
    "{texto}\n",
    " \n",
    "Análisis:\"\"\",\n",
    "    input_variables=[\"texto\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,  # Baja temperatura para respuestas más consistentes\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto_prueba = \"\"\"\n",
    "La nueva película de ciencia ficción 'Estrella Galáctica' es absolutamente \n",
    "espectacular. Los efectos visuales son impresionantes y la trama mantiene \n",
    "la tensión durante toda la película. Los actores principales entregan \n",
    "actuaciones convincentes que realmente te hacen creer en este mundo futurista.\n",
    "Sin duda una de las mejores películas del año.\n",
    "\"\"\"\n",
    " \n",
    "try:\n",
    "    # Invocar la cadena\n",
    "    resultado = chain.invoke({\"texto\": texto_prueba})\n",
    "    \n",
    "    # Acceder a los datos\n",
    "    print(\"=== RESULTADO DEL ANÁLISIS ===\")\n",
    "    print(f\"Resumen: {resultado.resumen}\")\n",
    "    print(f\"Sentimiento: {resultado.sentimiento}\")\n",
    "    print(f\"Palabras clave: {', '.join(resultado.palabras_clave)}\")\n",
    "    \n",
    "    # Exportar como JSON\n",
    "    print(\"\\n=== JSON RESULTANTE ===\")\n",
    "    print(resultado.model_dump_json(indent=2))\n",
    "    \n",
    "    # Exportar como diccionario\n",
    "    dict_resultado = resultado.model_dump()\n",
    "    print(f\"\\nTipo de objeto: {type(resultado)}\")\n",
    "    print(f\"Tipo de diccionario: {type(dict_resultado)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error durante el procesamiento: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
