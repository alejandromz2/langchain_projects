{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"ES Resume Alejandro Muñoz.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "for i, page in enumerate(pages):\n",
    "    print(f\"=== Pagina {i+1} ===\")\n",
    "    print(f\"Contenido: {page.page_content}\")\n",
    "    print(f\"Metadatos: {page.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://techmind.ac/', 'title': 'TechMind Academy - Cursos IA Generativa, Hacking Ético y Ciberseguridad | Santiago Hernández', 'description': 'Cursos especializados en IA Generativa, ChatGPT, LLMs, n8n, Hacking Ético y Ciberseguridad. 140k+ estudiantes. Aprende Inteligencia Artificial, automatización con IA, ciberseguridad y hacking ético con Santiago Hernández en TechMind Academy.', 'language': 'es'}, page_content='\\n\\n\\n\\n\\nTechMind Academy - Cursos IA Generativa, Hacking Ético y Ciberseguridad | Santiago Hernández\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://techmind.ac/\")\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/zrgh64d50kzgnjj977gd93vw0000gn/T/ipykernel_55988/4204475294.py:6: LangChainDeprecationWarning: The class `GoogleDriveLoader` was deprecated in LangChain 0.0.32 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-google-community package and should be used instead. To use it run `pip install -U :class:`~langchain-google-community` and import as `from :class:`~langchain_google_community import GoogleDriveLoader``.\n",
      "  loader = GoogleDriveLoader(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadatos {'source': 'https://drive.google.com/file/d/1hl9L8D4vUu-5oTN4oXWfkcCWVCf1L2bt/view', 'title': 'CONTRATO DE ARRENDAMIENTO DE PLAZA DE GARAJE.pdf', 'page': 1}\n",
      "Contenido: Cuarta. Renta y forma de pago  \n",
      "1. LA ARRENDATARIA abonará a EL ARRENDADOR una renta de CIENTO VEINTE EUROS \n",
      "(120,00  €) mensuales.  \n",
      "2. El pago se realizará por adelantado dentro de los cinco (5) primeros días  de cada mes \n",
      "mediante transferencia bancaria a la cuenta IBAN  ES75  0182  6200  8201  2345  6789 , o a \n",
      "la que EL ARRENDADOR designe por escrito.  \n",
      "3. La renta se actualizará anualmente conforme al porcentaje de variación del Índice de \n",
      "Precios al Consumo (IPC) General  publicado por el INE para los doce meses \n",
      "inmediatamente anteriores, aplicándose la primera actualización a partir del 1 de julio \n",
      "de 2026 . \n",
      "Quinta. Fianza y garantía adicional  \n",
      "1. En este acto, LA ARRENDATARIA entrega a EL ARRENDADOR la cantidad de \n",
      "DOSCIENTOS CUARENTA EUROS (240,00  €), equivalente a dos mensualidades de \n",
      "renta , en concepto de fianza legal , de acuerdo con el artículo  36.2  LAU. EL \n",
      "ARRENDADOR la depositará ante la Generalitat Valenciana – Servei Territorial \n",
      "d’Habitatge  en el plazo legal.  \n",
      "2. Se consigna igualmente un depósito de 50  € por el mando a distancia, que será \n",
      "devuelto a la entrega del mismo en perfecto estado.  \n",
      "Sexta. Gastos y servicios  \n",
      "1. Serán de cuenta de EL ARRENDADOR: el Impuesto sobre Bienes Inmuebles (IBI), las \n",
      "cuotas ordinarias de la comunidad de propietarios y el seguro de continente del garaje.  \n",
      "2. Serán de cuenta de LA ARRENDATARIA: los consumos eléctricos individualizables (si los \n",
      "hubiere) y los daños ocasionados por uso negligente.  \n",
      "Séptima. Acceso y normas de la comunidad  \n",
      "LA ARRENDATARIA se compromete a respetar las normas de régimen interior de la comunidad \n",
      "de propietarios, especialmente las relativas a velocidad máxima, sentido de circulación y \n",
      "prohibición de dejar el motor en marcha. El garaje dispone de acceso 24  horas  mediante el \n",
      "mando suministrado.  \n",
      "Octava. Cesión, subarriendo y transmisión  \n",
      "Queda expresamente prohibida la cesión del contrato, el subarriendo total o parcial y la \n",
      "transmisión del derecho de uso de la plaza sin autorización previa y por escrito de EL \n",
      "ARRENDADOR.  \n",
      "Novena. Responsabilidad y seguros  \n",
      "EL ARRENDADOR no responderá de los daños, robo o hurto que puedan sufrir los vehículos u \n",
      "objetos depositados en la plaza. LA ARRENDATARIA declara disponer de póliza de seguro que \n",
      "cubra, al menos, la responsabilidad civil obligatoria del vehículo estacionad o. \n",
      "Décima. Resolución y desistimiento  \n",
      "1. El contrato podrá resolverse por las causas generales del artículo  27 LAU, incluido el \n",
      "impago de renta o fianza y el incumplimiento de las normas de uso.  \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GoogleDriveLoader\n",
    "\n",
    "credentials_path = './credentials.json'\n",
    "token_path = './token.json'\n",
    "\n",
    "loader = GoogleDriveLoader(\n",
    "    folder_id = \"1lb1eOoS7l2ZaAISws6kMuSIey7BrAUgL\",\n",
    "    credentials_path=credentials_path,\n",
    "    token_path=token_path,\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\"Metadatos {documents[4].metadata}\")\n",
    "print(f\"Contenido: {documents[4].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextSplitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"El ingenioso hidalgo Don Quijote de la Mancha\" es una obra maestra de Miguel de Cervantes que narra las aventuras de Alonso Quijano, un noble que, tras leer numerosas novelas de caballería, decide convertirse en caballero andante bajo el nombre de Don Quijote. Acompañado de su fiel escudero, Sancho Panza, se embarca en una serie de peripecias donde confunde la realidad con la fantasía, enfrentándose a molinos de viento que cree son gigantes y defendiendo a los desvalidos.\n",
      "\n",
      "La obra explora varios temas fundamentales, como la locura de Don Quijote, cuya obsesión por las novelas de caballería lo lleva a perder el sentido de la realidad. La relación entre Don Quijote y Sancho Panza, que contrasta el idealismo del caballero con el pragmatismo del escudero, enriquece la narrativa y añade un tono humorístico y trágico. Cervantes también utiliza la figura de Don Quijote para criticar la sociedad de su tiempo, cuestionando las instituciones y los valores de la época, y explorando la delgada línea entre la realidad y la ilusión.\n",
      "\n",
      "La obra se divide en capítulos que relatan las diversas aventuras de Don Quijote. Desde su primera salida en busca de aventuras, donde se arma caballero, hasta sus encuentros con personajes peculiares y situaciones absurdas, cada capítulo añade profundidad a su carácter y a su búsqueda de ideales. A lo largo de la narrativa, se presentan episodios cómicos, como su confusión de una venta con un castillo, y momentos de reflexión sobre la naturaleza humana, la amistad y la lucha por un propósito en la vida.\n",
      "\n",
      "La estructura de la obra incluye una dedicatoria y un prólogo, así como una serie de capítulos que detallan las aventuras de Don Quijote y Sancho Panza, desde encuentros con cabreros y prisioneros hasta la búsqueda del yelmo de Mambrino y la liberación de prisioneros. La obra culmina en una rica mezcla de humor, crítica social y exploración de la locura, que se entrelazan para ofrecer una reflexión profunda sobre la condición humana.\n",
      "\n",
      "En resumen, \"Don Quijote de la Mancha\" es una obra que trasciende su tiempo, ofreciendo una mirada crítica y humorística sobre la locura, la amistad y la búsqueda de ideales, convirtiéndose en un clásico de la literatura universal.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Cargar el documento PDF\n",
    "loader = PyPDFLoader('./quijote.pdf')\n",
    "pages = loader.load()\n",
    "\n",
    "# Dividir el texto en chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=200 # Solapamiento de 200 caracteres\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "# 3. Pasar a un llm\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "summaries = []\n",
    "\n",
    "i = 0\n",
    "for chunk in chunks:\n",
    "    if i > 5:\n",
    "        break\n",
    "    response = llm.invoke(f'Haz un resumen de los puntos más importantes del siguiente texto {chunk.page_content}')\n",
    "    summaries.append(response.content)\n",
    "    i += 1\n",
    "\n",
    "final_summary = llm.invoke(f\"Combina y sintetiza estos resumenes en un resumen coherente y completo {\" \".join(summaries)}\")\n",
    "print(final_summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de los vectores 3072\n",
      "Similitud coseno entre vec1 y vec2: 0.818\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "texto1=\"La capital de Francia es París\"\n",
    "texto2 = \"Paris es la ciudad capital de Francia\"\n",
    "\n",
    "vec1 = embeddings.embed_query(texto1)\n",
    "vec2 = embeddings.embed_query(texto2)\n",
    "\n",
    "print(f\"Dimensión de los vectores {len(vec1)}\")\n",
    "\n",
    "cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1)* np.linalg.norm(vec2))\n",
    "\n",
    "print(f\"Similitud coseno entre vec1 y vec2: {cos_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bases de datos Vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El MultiQueryRetriever aborda las limitaciones de la búsqueda por similitud basada en distancia generando múltiples \"perspectivas\" alternativas de tu consulta original. En lugar de hacer una sola búsqueda, utiliza un LLM para crear varias versiones de la misma pregunta y luego combina los resultados.\n",
    "\n",
    "- Cuando tu consulta original puede ser interpretada de múltiples formas\n",
    "- Para mejorar la diversidad de resultados recuperados\n",
    "- En casos donde la consulta inicial podría no capturar todos los aspectos relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiQueryRetriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    " \n",
    "\n",
    "# Configurar el retriever\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    llm=llm\n",
    ")\n",
    " \n",
    "# Una pregunta se convierte en múltiples perspectivas\n",
    "results = multi_query_retriever.invoke(\"¿Cómo aprenden las redes neuronales?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ContextualCompressionRetriever: El Filtro Inteligente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los desafíos con la recuperación es que normalmente no conoces las consultas específicas que enfrentará tu sistema de almacenamiento de documentos cuando ingieres datos al sistema. Esto significa que la información más relevante para una consulta puede estar enterrada en un documento con mucho texto irrelevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce costos de llamadas a LLM al eliminar texto irrelevante\n",
    "- Mejora la calidad de las respuestas al proporcionar contexto más preciso\n",
    "- Permite pasar más documentos relevantes dentro del límite de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    " \n",
    "# Crear compresor que extrae solo contenido relevante\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    " \n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_store.as_retriever()\n",
    ")\n",
    " \n",
    "# Solo obtener las partes relevantes\n",
    "compressed_results = compression_retriever.invoke(\n",
    "    \"Explica el algoritmo de retropropagación\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    " \n",
    "# Pipeline: dividir -> filtrar redundantes -> comprimir por relevancia\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\".\")\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = LLMChainExtractor.from_llm(llm)\n",
    " \n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EnsembleRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El EnsembleRetriever soporta la combinación de resultados de múltiples retrievers. Los EnsembleRetrievers reordenan los resultados de los retrievers constituyentes basándose en el algoritmo de Fusión de Ranking Recíproco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casos de uso ideales\n",
    "- Búsquedas que requieren tanto precisión léxica como semántica\n",
    "- Documentos técnicos con terminología específica\n",
    "- Sistemas de búsqueda empresarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    " \n",
    "# Crear retriever BM25 (búsqueda por palabras clave)\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 5\n",
    " \n",
    "# Crear retriever vectorial (búsqueda semántica)\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embedding)\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    " \n",
    "# Combinar ambos con pesos\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.3, 0.7]  # 30% BM25, 70% vectorial\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El equilibrio perfecto\n",
    "\n",
    "El ParentDocumentRetriever logra ese equilibrio dividiendo y almacenando pequeños trozos de datos. Durante la recuperación, primero obtiene los trozos pequeños pero luego busca los IDs padre de esos trozos y devuelve esos documentos más grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embeddings precisos: Los chunks pequeños crean embeddings más representativos\n",
    "- Contexto completo: Devuelve documentos padre con contexto suficiente\n",
    "- Flexibilidad: Puedes ajustar el tamaño de chunks padre e hijo independientemente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    " \n",
    "# Splitter para documentos padre (chunks grandes)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    " \n",
    "# Splitter para documentos hijo (chunks pequeños para embedding)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    " \n",
    "# Vector store para los chunks pequeños\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"parent_docs\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    " \n",
    "# Almacenamiento para documentos padre\n",
    "store = InMemoryStore()\n",
    " \n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    " \n",
    "# Agregar documentos\n",
    "retriever.add_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelfQueryRetriever: Búsqueda Estructurada Inteligente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelfQueryRetriever utilizará un LLM para generar una consulta que es potencialmente estructurada, por ejemplo, puede construir filtros para la recuperación además de la selección habitual dirigida por similitud semántica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casos de uso\n",
    "- Bases de datos con metadatos ricos\n",
    "- Consultas que combinan contenido y filtros\n",
    "- Sistemas que requieren búsqueda estructurada automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    " \n",
    "# Definir metadatos de los documentos\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"El género de la película\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"El año de lanzamiento de la película\",\n",
    "        type=\"integer\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\",\n",
    "        description=\"La calificación de la película (1-10)\",\n",
    "        type=\"float\"\n",
    "    ),\n",
    "]\n",
    " \n",
    "document_content_description = \"Breve resumen de una película\"\n",
    " \n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    vectorstore=vectorstore,\n",
    "    document_content_description=document_content_description,\n",
    "    metadata_field_info=metadata_field_info,\n",
    ")\n",
    " \n",
    "# Consulta que se convertirá en filtros estructurados\n",
    "results = retriever.invoke(\"películas de ciencia ficción de después de 2010 con calificación alta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeWeightedVectorStoreRetriever: Memoria que Desvanece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para información sensible al tiempo\n",
    "\n",
    "Este retriever asigna mayor importancia a documentos más recientes, simulando cómo funciona la memoria humana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from datetime import datetime, timedelta\n",
    " \n",
    "tw_retriever = TimeWeightedVectorStoreRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    decay_rate=0.999,  # Qué tan rápido \"olvida\" documentos antiguos\n",
    "    k=5\n",
    ")\n",
    " \n",
    "# Los documentos más antiguos tendrán menos peso\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "tw_retriever.add_documents([Document(page_content=\"Noticia vieja\")], timestamps=[yesterday])\n",
    "tw_retriever.add_documents([Document(page_content=\"Noticia nueva\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnicas Avanzadas y Combinaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval con Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    " \n",
    "# Primer pase: recuperar muchos documentos\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    " \n",
    "# Segundo pase: reordenar con Cohere\n",
    "reranker = CohereRerank(\n",
    "    model=\"rerank-multilingual-v2.0\",\n",
    "    top_n=5\n",
    ")\n",
    " \n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker,\n",
    "    base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval MMR (Maximum Marginal Relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancear relevancia con diversidad\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,\n",
    "        \"lambda_mult\": 0.7  # Balance entre relevancia (1.0) y diversidad (0.0)\n",
    "    }\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
